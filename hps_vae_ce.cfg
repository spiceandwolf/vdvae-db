[run]
# run.name: Mandatory argument, used to identify runs for save and restore
name = vae_ce
# run.seed: seed that fixes all randomness in the project
seed = 2025

[data]
# Data paths.
dataset_path = '../dataset/household_power_consumption.txt'
pkl_path = './power/data.pkl'

[train]
# model saved path
output_dir = './saved_models/power_test/imputated_ce/test'
# batch_size
train_batch_size = 4096
eval_batch_size = 4096
# every attrabute's DVS
input_bins = None
# epochs / evaluating interval 
eval_interval_in_steps = 250
# total epochs
num_epochs = 10
learning_rate = 5e-4
warmup_iters = 250
# Adam/Radam/Adamax parameters
adam_beta1 = 0.9
adam_beta2 = 0.999

[encoder]
# strides -> blocks -> layers
# the number of encoder's strides. the last item should be False
n_encode_strides = [False,]
# the number of residual blocks prior to a connection with encoder
n_blocks_prior_encode_stride = [1,]
# the hidden dims of each block. the length should be equal to n_encode_strides + 1
hidden_dim_per_encode_block = [512, 512,]
# the number of residual layers in each block
n_residual_blocks_per_encode_block = [2,]
# the number of middle layers in the residual bottleneck block
n_residual_layers_per_block = [2,]
# the number of dims for input layer
input_dim = 1024
# the number of every attrabute's DVS
input_bins = None

[decoder]
# the times of downabstracting decoder's hidden unit
n_decode_strides = [False,]
# the number of residual blocks after to a connection with decoder
n_blocks_after_decode_stride = [1,]
# the hidden dims of each block 
hidden_dim_per_decode_block = [512, 512] 
# the latent dims of each block
latent_dim_per_decode_block = [64,]
# the number of residual layers in each block
n_residual_blocks_per_decode_block = [2,]
# the number of middle layers in the residual bottleneck block
n_residual_layers_per_block = [2,]
# the number of dims for output layer
output_dim = 1024
# the number of every attrabute's DVS
input_bins = None